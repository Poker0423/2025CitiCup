{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一.数据源集成实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 财报格式解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "class PDFParseError(Exception):\n",
    "    \"\"\"Custom exception for PDF parsing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class PDFParser:\n",
    "    def __init__(self):\n",
    "        self.table_settings = {\n",
    "            \"vertical_strategy\": \"text\", \n",
    "            \"horizontal_strategy\": \"text\",\n",
    "            \"keep_blank_chars\": True,\n",
    "            \"snap_tolerance\": 4\n",
    "        }\n",
    "\n",
    "    def extract_tables(self, file_path, page_range=None):\n",
    "        \"\"\"\n",
    "        支持多页表格的连续解析\n",
    "        参数：\n",
    "            page_range: 指定页码范围，例如 (0,3) 表示前3页\n",
    "        返回：\n",
    "            List[DataFrame] 表格列表\n",
    "        \"\"\"\n",
    "        tables = []\n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                total_pages = len(pdf.pages)\n",
    "                start, end = 0, total_pages-1\n",
    "                if page_range:\n",
    "                    start = max(0, page_range[0])\n",
    "                    end = min(total_pages-1, page_range[1])\n",
    "                \n",
    "                for i in range(start, end+1):\n",
    "                    page = pdf.pages[i]\n",
    "                    # 优化表格识别参数\n",
    "                    table = page.extract_table(self.table_settings)\n",
    "                    if table:\n",
    "                        # 处理跨页表格头重复问题\n",
    "                        if i > start and self._is_header_duplicate(tables[-1], table):\n",
    "                            table = table[1:]\n",
    "                        df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                        tables.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"PDF解析失败:{str(e)}\")\n",
    "            raise PDFParseError(\"PDF解析异常\") from e\n",
    "        return tables\n",
    "\n",
    "    def _is_header_duplicate(self, prev_df, current_table):\n",
    "        \"\"\"检测表格头是否重复\"\"\"\n",
    "        return list(prev_df.columns) == current_table[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "class ExcelParseError(Exception):\n",
    "    \"\"\"Custom exception for Excel parsing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class ExcelParser:\n",
    "    def parse_sheets(self, file_path, sheet_names=None):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            sheet_names: 指定需要解析的工作表名称列表\n",
    "        返回：\n",
    "            Dict[str: DataFrame] 工作表字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 预加载元数据\n",
    "            wb = load_workbook(file_path, read_only=True)\n",
    "            valid_sheets = sheet_names if sheet_names else wb.sheetnames\n",
    "            \n",
    "            # 读取数据\n",
    "            dfs = {}\n",
    "            for sheet in valid_sheets:\n",
    "                df = pd.read_excel(\n",
    "                    file_path,\n",
    "                    sheet_name=sheet,\n",
    "                    engine=\"openpyxl\",\n",
    "                    na_values=['NA', 'N/A'],\n",
    "                    dtype={'股票代码': str}  # 处理数字代码前导零问题\n",
    "                )\n",
    "                dfs[sheet] = self._clean_data(df)\n",
    "            return dfs\n",
    "        except Exception as e:\n",
    "            print(f\"Excel解析失败：{str(e)}\")\n",
    "            raise ExcelParseError(\"Excel解析异常\") from e\n",
    "\n",
    "    def _clean_data(self, df):\n",
    "        \"\"\"数据清洗\"\"\"\n",
    "        # 去除全空行列\n",
    "        df = df.dropna(how='all').T.dropna(how='all').T\n",
    "        # 处理合并单元格\n",
    "        df = df.ffill(axis=0)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "class HTMLParseError(Exception):\n",
    "    \"\"\"Custom exception for HTML parsing errors.\"\"\"\n",
    "    pass\n",
    "\n",
    "class HTMLParser:\n",
    "    def extract_data(self, file_path, table_css=None):\n",
    "        \"\"\"\n",
    "        参数：\n",
    "            table_css: CSS选择器（默认解析所有table标签）\n",
    "        返回：\n",
    "            List[DataFrame] 表格列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                soup = BeautifulSoup(f, 'html.parser')\n",
    "            \n",
    "            tables = soup.select(table_css) if table_css else soup.find_all('table')\n",
    "            dfs = []\n",
    "            for table in tables:\n",
    "                # 转换嵌套表格结构\n",
    "                df = self._convert_html_table(table)\n",
    "                if not df.empty:\n",
    "                    dfs.append(df)\n",
    "            return dfs\n",
    "        except Exception as e:\n",
    "            print(f\"HTML解析失败：{str(e)}\")\n",
    "            raise HTMLParseError(\"HTML解析异常\") from e\n",
    "\n",
    "    def _convert_html_table(self, table):\n",
    "        \"\"\"处理复杂表格结构\"\"\"\n",
    "        rows = []\n",
    "        for tr in table.find_all('tr'):\n",
    "            cells = []\n",
    "            for td in tr.find_all(['th', 'td']):\n",
    "                # 处理跨行列\n",
    "                rowspan = int(td.get('rowspan', 1))\n",
    "                colspan = int(td.get('colspan', 1))\n",
    "                cells.append({\n",
    "                    'text': re.sub(r'\\s+', ' ', td.text).strip(),\n",
    "                    'rowspan': rowspan,\n",
    "                    'colspan': colspan\n",
    "                })\n",
    "            rows.append(cells)\n",
    "        \n",
    "        # 构建二维表格结构\n",
    "        matrix = self._build_matrix(rows)\n",
    "        return pd.DataFrame(matrix[1:], columns=matrix[0])\n",
    "\n",
    "    def _build_matrix(self, rows):\n",
    "        \"\"\"处理行列合并\"\"\"\n",
    "        # 实现动态矩阵构建逻辑（此处省略具体实现）\n",
    "        # 返回二维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialReportParser:\n",
    "    def __init__(self):\n",
    "        # 初始化解析器\n",
    "        pass\n",
    "\n",
    "    def parse(self, file_path):\n",
    "        file_type = self._detect_file_type(file_path)\n",
    "        # 根据文件类型调用不同的解析方法\n",
    "        if file_type == 'pdf':\n",
    "            return self.pdf_parser.extract_tables(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            return self.excel_parser.parse_sheets(file_path)\n",
    "        elif file_type == 'html':\n",
    "            return self.html_parser.extract_data(file_path)\n",
    "\n",
    "    def _detect_file_type(self, file_path):\n",
    "        # 通过文件扩展名或其他方式来判断文件类型\n",
    "        if file_path.endswith('.pdf'):\n",
    "            return 'pdf'\n",
    "        elif file_path.endswith('.xls') or file_path.endswith('.xlsx'):\n",
    "            return 'excel'\n",
    "        elif file_path.endswith('.html'):\n",
    "            return 'html'\n",
    "        else:\n",
    "            return 'unknown'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 数据标准化处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "class FinancialDataNormalizer:\n",
    "    def __init__(self, field_mapping=None, unit_conversion=None, time_format=\"%Y-%m-%d %H:%M:%S\"):\n",
    "        \"\"\"\n",
    "        初始化数据标准化处理器\n",
    "\n",
    "        :param field_mapping: 字段映射字典，例如 {\"原始字段名\": \"标准字段名\"}\n",
    "        :param unit_conversion: 单位转换字典，例如 {\"美元\": 1, \"欧元\": 1.1}，\n",
    "                                用于将不同币种或数值单位转换到统一的标准单位\n",
    "        :param time_format: 原始时间字符串的格式，用于时间格式化\n",
    "        \"\"\"\n",
    "        self.field_mapping = field_mapping or {}\n",
    "        self.unit_conversion = unit_conversion or {}\n",
    "        self.time_format = time_format\n",
    "\n",
    "    def normalize(self, raw_data):\n",
    "        \"\"\"\n",
    "        对原始数据进行标准化处理：\n",
    "        1. 字段映射标准化\n",
    "        2. 数值单位统一\n",
    "        3. 时间序列对齐\n",
    "        4. 数据质量检查\n",
    "\n",
    "        :param raw_data: 原始数据字典\n",
    "        :return: 标准化后的数据字典\n",
    "        \"\"\"\n",
    "        # 1. 字段映射标准化\n",
    "        mapped_data = self._map_to_standard_fields(raw_data)\n",
    "        \n",
    "        # 2. 数值单位统一\n",
    "        unified_data = self._unify_units(mapped_data)\n",
    "        \n",
    "        # 3. 时间序列对齐\n",
    "        aligned_data = self._align_time_series(unified_data)\n",
    "        \n",
    "        # 4. 数据质量检查\n",
    "        validated_data = self._validate_data(aligned_data)\n",
    "        \n",
    "        return validated_data\n",
    "\n",
    "    def _map_to_standard_fields(self, raw_data):\n",
    "        \"\"\"\n",
    "        将原始数据字段映射到标准字段。\n",
    "\n",
    "        :param raw_data: 原始数据字典\n",
    "        :return: 字段名称已映射为标准名称的新数据字典\n",
    "        \"\"\"\n",
    "        mapped_data = {}\n",
    "        for key, value in raw_data.items():\n",
    "            std_key = self.field_mapping.get(key, key)\n",
    "            mapped_data[std_key] = value\n",
    "        return mapped_data\n",
    "\n",
    "    def _unify_units(self, data):\n",
    "        \"\"\"\n",
    "        统一数值单位，比如将不同币种或计量单位转换为标准单位。\n",
    "        假设相关数值数据以元组形式出现：(amount, unit)\n",
    "\n",
    "        :param data: 数据字典\n",
    "        :return: 数值统一后的数据字典\n",
    "        \"\"\"\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, tuple) and len(value) == 2:\n",
    "                amount, unit = value\n",
    "                conversion_factor = self.unit_conversion.get(unit, 1)\n",
    "                data[key] = amount * conversion_factor\n",
    "        return data\n",
    "\n",
    "    def _align_time_series(self, data):\n",
    "        \"\"\"\n",
    "        对时间数据进行格式化处理，确保时间序列的一致性。\n",
    "        假设时间字段的标准名称为 \"timestamp\"\n",
    "\n",
    "        :param data: 数据字典\n",
    "        :return: 时间格式统一后的数据字典\n",
    "        \"\"\"\n",
    "        if \"timestamp\" in data:\n",
    "            try:\n",
    "                data[\"timestamp\"] = datetime.datetime.strptime(data[\"timestamp\"], self.time_format)\n",
    "            except Exception as e:\n",
    "                print(f\"时间转换错误: {e}\")\n",
    "                data[\"timestamp\"] = None\n",
    "        return data\n",
    "\n",
    "    def _validate_data(self, data):\n",
    "        \"\"\"\n",
    "        进行数据质量检查，例如关键字段是否存在、数值是否合理等。\n",
    "        此处以检查 \"amount\" 字段为例，判断其是否为正值。\n",
    "\n",
    "        :param data: 数据字典\n",
    "        :return: 增加数据有效性标记后的数据字典\n",
    "        \"\"\"\n",
    "        is_valid = True\n",
    "        if \"amount\" in data:\n",
    "            if data[\"amount\"] is None or data[\"amount\"] <= 0:\n",
    "                is_valid = False\n",
    "        \n",
    "        # 你可以在此处添加更多的校验逻辑\n",
    "        data[\"is_valid\"] = is_valid\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1.3提关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.analyse\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# 下载 nltk 需要的资源（如果未安装，请先运行一次）\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "class ExchangeRateRiskKeywordsExtractor:\n",
    "    def __init__(self, topK=10, stop_words_file=None):\n",
    "        \"\"\"\n",
    "        初始化关键词提取器，支持中英文\n",
    "        \n",
    "        :param topK: 提取的关键词数量\n",
    "        :param stop_words_file: 可选的中文停用词文件路径\n",
    "        \"\"\"\n",
    "        self.topK = topK\n",
    "        # 设置中文停用词\n",
    "        if stop_words_file:\n",
    "            jieba.analyse.set_stop_words(stop_words_file)\n",
    "        # 获取英文停用词表\n",
    "        self.english_stopwords = set(stopwords.words('english'))\n",
    "    \n",
    "    def extract_keywords(self, risk_text):\n",
    "        \"\"\"\n",
    "        从混合的中英文汇率风险文本中提取关键词\n",
    "        \"\"\"\n",
    "        # 1. 提取中文关键词\n",
    "        cn_keywords = jieba.analyse.extract_tags(risk_text, topK=self.topK, withWeight=False)\n",
    "\n",
    "        # 2. 提取英文关键词\n",
    "        en_keywords = self.extract_english_keywords(risk_text)\n",
    "\n",
    "        # 3. 合并中英文关键词\n",
    "        combined_keywords = list(set(cn_keywords + en_keywords))\n",
    "\n",
    "        return combined_keywords\n",
    "\n",
    "    def extract_english_keywords(self, text):\n",
    "        \"\"\"\n",
    "        提取英文关键词（去除停用词、标点，并转换为小写）\n",
    "        \"\"\"\n",
    "        words = word_tokenize(text)  # 分词\n",
    "        filtered_words = [\n",
    "            word.lower() for word in words\n",
    "            if word.isalnum() and word.lower() not in self.english_stopwords\n",
    "        ]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.RAG系统"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1新闻文档分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (4.45.2)\n",
      "Requirement already satisfied: tqdm in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (0.24.6)\n",
      "Requirement already satisfied: Pillow in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: filelock in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tangguochuan/anaconda3/envs/hq/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class NewsChunker:\n",
    "    def __init__(self):\n",
    "        # 加载多语言模型\n",
    "        self.nlp = spacy.load('xx_ent_wiki_sm')  # 多语言模型\n",
    "        self.min_chunk_size = 100  # 最小分块字符数\n",
    "        self.max_chunk_size = 500  # 最大分块字符数\n",
    "        self.paragraph_sep = [\"\\n\\n\", \"。\", \"！\", \"？\", \".\", \"!\", \"?\"]  # 中英文段落分隔符\n",
    "        self.exchange_rate_markers = [\"汇率\", \"exchange rate\", \"USD/CNY\", \"EUR/USD\", \"JPY/CNY\", \"美元/人民币\"]  # 汇率关键词\n",
    "        self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "    def chunk_news(self, news_text):\n",
    "        \"\"\"主分块方法\"\"\"\n",
    "        doc = self.nlp(news_text)\n",
    "        semantic_paragraphs = self._get_semantic_paragraphs(doc)\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        \n",
    "        for para in semantic_paragraphs:\n",
    "            para_len = len(para)\n",
    "            \n",
    "            # 处理超长段落的分割\n",
    "            if para_len > self.max_chunk_size:\n",
    "                sub_paragraphs = self._split_long_paragraph(para)\n",
    "                for sub_para in sub_paragraphs:\n",
    "                    chunks.extend(self._process_paragraph(sub_para))\n",
    "                continue\n",
    "                \n",
    "            # 动态合并逻辑\n",
    "            if self._should_start_new_chunk(current_size, para_len):\n",
    "                if current_chunk:\n",
    "                    chunks.append(self._create_chunk(current_chunk))\n",
    "                current_chunk = [para]\n",
    "                current_size = para_len\n",
    "            else:\n",
    "                current_chunk.append(para)\n",
    "                current_size += para_len\n",
    "                \n",
    "        # 处理最后一个块\n",
    "        if current_chunk:\n",
    "            chunks.append(self._create_chunk(current_chunk))\n",
    "            \n",
    "        return chunks\n",
    "\n",
    "    def _get_semantic_paragraphs(self, doc):\n",
    "        \"\"\"语义段落识别\"\"\"\n",
    "        paragraphs = []\n",
    "        current_para = []\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            # 检测段落分隔符\n",
    "            if self._is_paragraph_boundary(sent):\n",
    "                if current_para:\n",
    "                    paragraphs.append(\"\".join(current_para))\n",
    "                    current_para = []\n",
    "                continue\n",
    "                \n",
    "            current_para.append(sent.text)\n",
    "            \n",
    "            # 基于依存关系的段落分割\n",
    "            if self._has_discourse_marker(sent):\n",
    "                paragraphs.append(\"\".join(current_para))\n",
    "                current_para = []\n",
    "        \n",
    "        # 处理最后一个段落\n",
    "        if current_para:\n",
    "            paragraphs.append(\"\".join(current_para))\n",
    "            \n",
    "        return paragraphs\n",
    "\n",
    "    def _is_paragraph_boundary(self, sent):\n",
    "        \"\"\"检测显式段落分隔符\"\"\"\n",
    "        return any(sep in sent.text for sep in self.paragraph_sep)\n",
    "\n",
    "    def _has_discourse_marker(self, sent):\n",
    "        \"\"\"检测语篇标记词（转折、因果等连接词）\"\"\"\n",
    "        markers = [\"然而\", \"因此\", \"同时\", \"另一方面\", \"尽管如此\", \"however\", \"therefore\", \"meanwhile\", \"on the other hand\"]\n",
    "        for token in sent:\n",
    "            if token.text in markers and token.dep_ == \"mark\":\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _should_start_new_chunk(self, current_size, new_para_size):\n",
    "        \"\"\"动态分块决策逻辑\"\"\"\n",
    "        # 强制分块条件\n",
    "        if current_size + new_para_size > self.max_chunk_size:\n",
    "            return True\n",
    "        # 语义完整性保护：不合并包含不同实体的段落\n",
    "        if current_size > self.min_chunk_size and new_para_size > self.min_chunk_size:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _split_long_paragraph(self, paragraph):\n",
    "        \"\"\"处理超长段落的分割\"\"\"\n",
    "        doc = self.nlp(paragraph)\n",
    "        split_points = []\n",
    "        \n",
    "        # 寻找自然分割点\n",
    "        for sent in doc.sents:\n",
    "            if len(sent) > 50:  # 长句子优先作为分割点\n",
    "                split_points.append(sent.end)\n",
    "        \n",
    "        # 动态生成子段落\n",
    "        sub_paragraphs = []\n",
    "        start = 0\n",
    "        for end in split_points:\n",
    "            sub_para = doc[start:end].text\n",
    "            if len(sub_para) > self.min_chunk_size:\n",
    "                sub_paragraphs.append(sub_para)\n",
    "                start = end\n",
    "        # 处理剩余部分\n",
    "        if start < len(doc):\n",
    "            sub_paragraphs.append(doc[start:].text)\n",
    "            \n",
    "        return sub_paragraphs\n",
    "\n",
    "    def _create_chunk(self, paragraphs):\n",
    "        \"\"\"创建最终分块\"\"\"\n",
    "        chunk_text = \"\\n\".join(paragraphs)\n",
    "        return {\n",
    "            \"text\": chunk_text,\n",
    "            \"length\": len(chunk_text),\n",
    "            \"paragraph_count\": len(paragraphs),\n",
    "            \"entities\": self._extract_entities(chunk_text)\n",
    "        }\n",
    "\n",
    "    def _extract_entities(self, text):\n",
    "        \"\"\"提取金融实体信息\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        entities = []\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            # 提取机构、货币、日期等实体\n",
    "            if ent.label_ in [\"ORG\", \"MONEY\", \"DATE\", \"GPE\"]:\n",
    "                entities.append({\n",
    "                    \"text\": ent.text,\n",
    "                    \"label\": ent.label_,\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char\n",
    "                })\n",
    "            # 提取汇率相关实体（如USD/CNY）\n",
    "            if ent.label_ == \"MONEY\" and \"/\" in ent.text:\n",
    "                entities.append({\n",
    "                    \"text\": ent.text,\n",
    "                    \"label\": \"EXCHANGE_RATE\",\n",
    "                    \"start\": ent.start_char,\n",
    "                    \"end\": ent.end_char\n",
    "                })\n",
    "        \n",
    "        return entities\n",
    "    def vectorize_text(self, chunks: List[Dict]) -> np.ndarray:\n",
    "        \"\"\"将文本分块向量化\"\"\"\n",
    "        texts = [chunk[\"text\"] for chunk in chunks]\n",
    "        return self.embedding_model.encode(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2数据分块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "class DataChunker:\n",
    "    def __init__(self, \n",
    "                 window_size: int = 7,\n",
    "                 vectorization_method: str = \"statistical\",\n",
    "                 text_template: str = \"日期范围：{start_date}至{end_date}，{column}平均值为{mean:.2f}，标准差为{std:.2f}\"):\n",
    "        \"\"\"\n",
    "        结构化数据分块与向量化处理器\n",
    "        \n",
    "        :param window_size: 时间窗口大小（适用于时间序列）\n",
    "        :param vectorization_method: 向量化方法 [\"statistical\", \"text_embedding\"]\n",
    "        :param text_template: 文本描述模板（用于text_embedding模式）\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.vectorization_method = vectorization_method\n",
    "        self.text_template = text_template\n",
    "        \n",
    "        # 加载多语言文本嵌入模型（用于text_embedding模式）\n",
    "        if vectorization_method == \"text_embedding\":\n",
    "            self.embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "    def chunk_data(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 timestamp_col: str = \"date\") -> List[Dict]:\n",
    "        \"\"\"\n",
    "        数据分块方法\n",
    "        \n",
    "        :param df: 输入数据框\n",
    "        :param timestamp_col: 时间戳列名\n",
    "        :return: 分块列表，每个分块包含数据和元数据\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        # 按时间窗口分块\n",
    "        for i in range(0, len(df), self.window_size):\n",
    "            chunk_df = df.iloc[i:i+self.window_size]\n",
    "            \n",
    "            # 计算元数据\n",
    "            metadata = {\n",
    "                \"start_date\": chunk_df[timestamp_col].min(),\n",
    "                \"end_date\": chunk_df[timestamp_col].max(),\n",
    "                \"columns\": chunk_df.columns.tolist(),\n",
    "                \"row_count\": len(chunk_df)\n",
    "            }\n",
    "            \n",
    "            chunks.append({\n",
    "                \"data\": chunk_df,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def vectorize(self, \n",
    "                 chunks: List[Dict],\n",
    "                 target_columns: List[str] = [\"USD/CNY\"]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        数据向量化方法\n",
    "        \n",
    "        :param chunks: 分块列表\n",
    "        :param target_columns: 需要处理的数值列\n",
    "        :return: 向量数组 (n_chunks, n_features)\n",
    "        \"\"\"\n",
    "        vectors = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            df = chunk[\"data\"]\n",
    "            metadata = chunk[\"metadata\"]\n",
    "            \n",
    "            if self.vectorization_method == \"statistical\":\n",
    "                # 统计特征向量化\n",
    "                vec = []\n",
    "                for col in target_columns:\n",
    "                    vec.extend([\n",
    "                        df[col].mean(),    # 均值\n",
    "                        df[col].std(),     # 标准差\n",
    "                        df[col].max(),     # 最大值\n",
    "                        df[col].min(),     # 最小值\n",
    "                        df[col].diff().mean()  # 趋势\n",
    "                    ])\n",
    "                vectors.append(vec)\n",
    "                \n",
    "            elif self.vectorization_method == \"text_embedding\":\n",
    "                # 生成文本描述后向量化\n",
    "                descriptions = []\n",
    "                for col in target_columns:\n",
    "                    desc = self.text_template.format(\n",
    "                        start_date=metadata[\"start_date\"],\n",
    "                        end_date=metadata[\"end_date\"],\n",
    "                        column=col,\n",
    "                        mean=df[col].mean(),\n",
    "                        std=df[col].std()\n",
    "                    )\n",
    "                    descriptions.append(desc)\n",
    "                \n",
    "                # 拼接所有列的描述\n",
    "                full_text = \"。\".join(descriptions)\n",
    "                vec = self.embedding_model.encode([full_text])[0].tolist()\n",
    "                vectors.append(vec)\n",
    "                \n",
    "        return np.array(vectors)\n",
    "\n",
    "    def get_chunk_descriptions(self,\n",
    "                              chunks: List[Dict],\n",
    "                              target_columns: List[str] = [\"USD/CNY\"]) -> List[str]:\n",
    "        \"\"\"\n",
    "        生成分块文本描述（用于与文本分块统一检索）\n",
    "        \n",
    "        :return: 文本描述列表\n",
    "        \"\"\"\n",
    "        descriptions = []\n",
    "        for chunk in chunks:\n",
    "            df = chunk[\"data\"]\n",
    "            metadata = chunk[\"metadata\"]\n",
    "            \n",
    "            descs = []\n",
    "            for col in target_columns:\n",
    "                desc = self.text_template.format(\n",
    "                    start_date=metadata[\"start_date\"],\n",
    "                    end_date=metadata[\"end_date\"],\n",
    "                    column=col,\n",
    "                    mean=df[col].mean(),\n",
    "                    std=df[col].std()\n",
    "                )\n",
    "                descs.append(desc)\n",
    "            \n",
    "            descriptions.append(\"。\".join(descs))\n",
    "        \n",
    "        return descriptions\n",
    "\n",
    "    def hybrid_retrieve(self,\n",
    "                       text_vectors: np.ndarray,\n",
    "                       data_vectors: np.ndarray,\n",
    "                       query: str,\n",
    "                       top_k: int = 3,\n",
    "                       alpha: float = 0.5) -> List[int]:\n",
    "        \"\"\"\n",
    "        混合检索方法（文本与数据）\n",
    "        \n",
    "        :param text_vectors: 文本分块向量\n",
    "        :param data_vectors: 数据分块向量\n",
    "        :param query: 查询文本\n",
    "        :param top_k: 返回结果数量\n",
    "        :param alpha: 文本权重 (0-1)\n",
    "        :return: 相关分块索引列表\n",
    "        \"\"\"\n",
    "        # 查询向量化\n",
    "        query_vec = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # 计算相似度\n",
    "        text_sim = np.dot(text_vectors, query_vec)\n",
    "        data_sim = np.dot(data_vectors, query_vec)\n",
    "        \n",
    "        # 混合相似度\n",
    "        combined_sim = alpha * text_sim + (1 - alpha) * data_sim\n",
    "        \n",
    "        # 获取Top-K索引\n",
    "        sorted_indices = np.argsort(combined_sim)[::-1]\n",
    "        return sorted_indices[:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.3 构建混合索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class HybridIndexer:\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.chunks = []  # 存储所有分块的元数据\n",
    "\n",
    "    def build_index(self, text_chunks: List[Dict], data_chunks: List[Dict]):\n",
    "        # 合并文本和数据分块\n",
    "        all_chunks = text_chunks + data_chunks\n",
    "        self.chunks = all_chunks\n",
    "        \n",
    "        # 提取所有嵌入向量\n",
    "        embeddings = []\n",
    "        for chunk in all_chunks:\n",
    "            embeddings.append(chunk[\"embedding\"])\n",
    "        embeddings = np.array(embeddings).astype('float32')\n",
    "        \n",
    "        # 创建FAISS索引\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5) -> List[Dict]:\n",
    "        # 查询向量化\n",
    "        query_embedding = self.embedding_model.encode([query])[0]\n",
    "        \n",
    "        # FAISS检索\n",
    "        distances, indices = self.index.search(\n",
    "            np.array([query_embedding]).astype('float32'), \n",
    "            top_k\n",
    "        )\n",
    "        \n",
    "        # 返回结果分块\n",
    "        return [self.chunks[i] for i in indices[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "class GPT4Generator:\n",
    "    def __init__(self, api_key: str):\n",
    "        openai.api_key = api_key\n",
    "\n",
    "    def generate(self, prompt: str, max_tokens: int = 500) -> str:\n",
    "        \"\"\"调用 GPT-4 生成答案\"\"\"\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个金融分析师，根据提供的上下文生成专业、准确的回答。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=0.3  # 控制生成结果的随机性\n",
    "        )\n",
    "        return response.choices[0].message[\"content\"]\n",
    "\n",
    "class ConversationManager:\n",
    "    def __init__(self, max_tokens: int = 8000, recent_window: int = 5):\n",
    "        \"\"\"\n",
    "        :param max_tokens: 最大上下文长度（tokens）\n",
    "        :param recent_window: 最近对话窗口大小\n",
    "        \"\"\"\n",
    "        self.max_tokens = max_tokens\n",
    "        self.recent_window = recent_window\n",
    "        self.conversation_history = []  # 存储对话历史\n",
    "\n",
    "    def add_message(self, role: str, content: str):\n",
    "        \"\"\"添加对话消息\"\"\"\n",
    "        self.conversation_history.append({\"role\": role, \"content\": content})\n",
    "\n",
    "    def truncate_context(self) -> List[Dict]:\n",
    "        \"\"\"截断上下文，确保不超过最大长度\"\"\"\n",
    "        # 优先保留最近的对话\n",
    "        recent_messages = self.conversation_history[-self.recent_window:]\n",
    "        \n",
    "        # 计算当前 tokens 数量\n",
    "        current_tokens = sum(len(msg[\"content\"].split()) for msg in recent_messages)\n",
    "        \n",
    "        # 如果未超过最大长度，直接返回\n",
    "        if current_tokens <= self.max_tokens:\n",
    "            return recent_messages\n",
    "        \n",
    "        # 如果超过最大长度，逐步移除最早的消息\n",
    "        truncated_messages = recent_messages.copy()\n",
    "        while current_tokens > self.max_tokens and len(truncated_messages) > 1:\n",
    "            removed_message = truncated_messages.pop(0)  # 移除最早的消息\n",
    "            current_tokens -= len(removed_message[\"content\"].split())\n",
    "        \n",
    "        return truncated_messages\n",
    "\n",
    "    def get_context(self) -> str:\n",
    "        \"\"\"获取截断后的上下文\"\"\"\n",
    "        truncated_messages = self.truncate_context()\n",
    "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in truncated_messages])\n",
    "\n",
    "\n",
    "class FinancialRAG:\n",
    "    def __init__(self, indexer: HybridIndexer, gpt4_generator: GPT4Generator):\n",
    "        self.indexer = indexer\n",
    "        self.gpt4_generator = gpt4_generator\n",
    "        self.conversation_manager = ConversationManager()\n",
    "\n",
    "    def generate_answer(self, query: str) -> str:\n",
    "        # 添加用户问题到对话历史\n",
    "        self.conversation_manager.add_message(\"user\", query)\n",
    "        \n",
    "        # 检索相关分块\n",
    "        relevant_chunks = self.indexer.search(query)\n",
    "        context = self._format_context(relevant_chunks)\n",
    "        \n",
    "        # 获取截断后的对话历史\n",
    "        conversation_context = self.conversation_manager.get_context()\n",
    "        \n",
    "        # 构建 GPT-4 的输入提示\n",
    "        prompt = f\"\"\"\n",
    "        对话历史：\n",
    "        {conversation_context}\n",
    "        \n",
    "        上下文：\n",
    "        {context}\n",
    "        \n",
    "        问题：{query}\n",
    "        请根据上下文和对话历史生成回答。\n",
    "        \"\"\"\n",
    "        \n",
    "        # 调用 GPT-4 生成答案\n",
    "        answer = self.gpt4_generator.generate(prompt)\n",
    "        \n",
    "        # 添加生成的答案到对话历史\n",
    "        self.conversation_manager.add_message(\"assistant\", answer)\n",
    "        return answer\n",
    "\n",
    "    def _format_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"格式化检索到的分块为 GPT-4 的输入上下文\"\"\"\n",
    "        context = []\n",
    "        for chunk in chunks:\n",
    "            if chunk[\"type\"] == \"text\":\n",
    "                context.append(f\"[新闻] {chunk['content']}\")\n",
    "            elif chunk[\"type\"] == \"data\":\n",
    "                context.append(f\"[数据] {chunk['metadata']['description']}\")\n",
    "        return \"\\n\".join(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
